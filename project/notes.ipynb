{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = \\\n",
    "        df_weather\\\n",
    "            .rename(columns={'0': 'station', '1': 'datetime', '2': 'obs_type', '3': 'obs_value'})\\\n",
    "            .query(\"obs_type == 'TMAX'\")\\\n",
    "            .assign(obs_value=lambda df: df['obs_value']/10)\\\n",
    "            .sort_values(by=['station', 'datetime'], ascending=[True, False])\\\n",
    "            .reset_index(drop=True)\\\n",
    "            .copy() \n",
    "\n",
    ".str[0:2]\n",
    "pd.qcut(df_weather_period['obs_value'], [0, 0.1, 0.9, 1], labels=[\"cold\", \"medium\", \"hot\"])\n",
    "\n",
    "df_weather_period['tmax_mean'] \\\n",
    "    = df_weather_period\\\n",
    "        .groupby(['month', 'year', 'station'])['obs_value']\\\n",
    "            .transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array().unique, nunique\n",
    "cut, qcut\n",
    "diff\n",
    "cumsum \n",
    "nlargest, nsmallet\n",
    "idxmin, idxmax \n",
    "corr \n",
    "\n",
    "pd.Series().str.lower() \n",
    "contains()\n",
    "astype() \n",
    "pd.Categorical()\n",
    "cut, qcut \n",
    "pd.get_dummies()\n",
    "pd.to_datetime\n",
    "\n",
    "pip install yfinance\n",
    "import yfinance as yf\n",
    "\n",
    "np.random.seed()\n",
    "np.round()\n",
    "to_dict() \n",
    "tolist() \n",
    "append()\n",
    "pd.DataFrame.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename().assign().sort_values().reset_index()\n",
    "lambda x1, x2, x3: x1+x2*x3 \n",
    "df.isnull()\n",
    "df.replace()\n",
    "df.fillna\n",
    "df.dropna\n",
    "df.duplicated() \n",
    "df.drop_duplicates()\n",
    "merge \n",
    "set_index \n",
    "join \n",
    "concat \n",
    "groupby \n",
    "stack, unstack \n",
    "melt, pivot \n",
    "read_csv... \n",
    "query "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.subplots()\n",
    "plt.style.use('ggplot')\n",
    "plt.rc('figure', figsize=(6, 3)) # set default size of plots\n",
    "font_options = {'family' : 'monospace', # define default font options\n",
    "                'weight' : 'bold',\n",
    "                'size'   : 12}\n",
    "\n",
    "plt.rc('font', **font_options) # set default font options\n",
    "ax.hist(tb);\n",
    "props = {\n",
    "    'title': 'Distribution of bill size',\n",
    "    'xlabel': 'Total bill ($)',\n",
    "    'ylabel': 'Count',\n",
    "    'xlim': [0, 60]\n",
    "}\n",
    "ax.set(**props)\n",
    "plt.rcdefaults()\n",
    "\n",
    "sns.histplot(tb);\n",
    "ax.set(xlabel='Total bill ($)');\n",
    "ax = sns.distplot(tb, hist_kws={'cumulative': True}, kde_kws={'cumulative': True}) \n",
    "sns.countplot(x='sex', data=tips);\n",
    "ax.pie(sizes, labels=['Male', 'Female'], autopct='%1.2f%%');  # Make pie representation\n",
    "ax.scatter(x=tips['total_bill'], y=tips['tip'])\n",
    "data = np.concatenate((Y,X),axis=1)\n",
    "ax = sns.jointplot(x='total_bill', y='tip', data=tips, kind='kde', size=3) # hex, reg, resid\n",
    "ax = sns.lmplot(x='total_bill', y='tip', data=tips, size=3, aspect=2.5) \n",
    "ax = sns.catplot(x=\"sex\", y=\"tip\", hue=\"time\", kind=\"swarm\", data=tips, size=3)\n",
    "ax = sns.violinplot(x='time', y='tip', data=tips, hue='sex')\n",
    "ax = sns.barplot(x='time', y='tip',  data=tips, hue='sex')  #ci=None\n",
    "ax = sns.lmplot(x='total_bill', y= 'tip', hue='sex', data=tips, size=3)\n",
    "sns.barplot(x='day', y='tip', data=tips, ax=ax[0]) \n",
    "sns.pairplot(tips, vars = ['size', 'tip', 'total_bill', 'sex'] ,height=2.3, size=1.4); # make hist and scatter for all numeric variables\n",
    "\n",
    "f,ax = plt.subplots()\n",
    "\n",
    "ax.hist( tips['total_bill']) # Choose histogram\n",
    "\n",
    "ax.set_title('Distribution of total bill') # Choose title for plot\n",
    "ax.title.set_fontsize(20) # Choose title font size\n",
    "\n",
    "ax.set_xlabel('Total bill, $') # Choose title x-axis\n",
    "ax.xaxis.label.set_fontsize(16) # Choose font size for a-axis title\n",
    "\n",
    "ax.set_ylabel('Count') # Choose title y-axis\n",
    "ax.yaxis.label.set_fontsize(16) # Choose font size for y-axis title\n",
    "    \n",
    "for item in ax.get_yticklabels()+ax.get_xticklabels():\n",
    "    item.set_fontsize(12) # Choose size of ticks\n",
    "sns.despine()\n",
    "\n",
    "plt.savefig('ex_323_plot.pdf')\n",
    "\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
    "          fancybox=True, shadow=True, ncol=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "fp_data = Path.cwd() / \"data\" # create path\n",
    "Path.mkdir(fp_data, exist_ok=True) # create subfolder\n",
    "ZipFile('data.zip', 'r').extractall(fp_data) # extract zip-file to data-folder\n",
    "\n",
    "with open('data/ghcnd-stations-column-metadata.txt', encoding=\"utf-8\") as f:\n",
    "    column_metadata = f.read()\n",
    ".split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['cat1','cat2'])[['val1','val2']].agg(['mean','median'])\n",
    "df.groupby(['cat1', 'cat2'])['val1'].transform('median')\n",
    "key_value_pairs = list(zip(keys, values))\n",
    "\n",
    "import requests\n",
    "response = requests.get('https://api.punkapi.com/v2/beers/1')\n",
    "response.json()[0]['name']\n",
    "\n",
    "str1.upper()\n",
    "str1.replace('po', 'ma')\n",
    "s1.capitalize()\n",
    "\n",
    "with open('to_do_list.txt', 'w') as f:\n",
    "    for i in range(len(to_do)):\n",
    "        f.write(to_do[i]+'\\n')\n",
    "\n",
    "import json\n",
    "\n",
    "with open('my_file.JSON', 'w') as f:\n",
    "    my_json_str = json.dumps(words) # convert dictonary to string with JSON formatting\n",
    "    f.write(my_json_str) # write the string to file\n",
    "\n",
    "with open('my_file.JSON', 'r') as f:\n",
    "    print(f.read()) # read the string from file\n",
    "\n",
    "response_json = response.json() # convert response to a list of dicts\n",
    "\n",
    "import pprint #Data pretty printer\n",
    "pprint.pprint(response.json()) #Everything is aranged alphabetically and in appropriate levels\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "import re \n",
    "\n",
    "rehits = re.findall(r'href=[\\'\"]?([^\\'\" >]+)', html)\n",
    ".zfill(2)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "list_htmls = []\n",
    "for url in tqdm.tqdm(links): #Track the time left before completing the loop\n",
    "    response = requests.get(url, header=dict)\n",
    "    html = response.text\n",
    "    list_htmls.append(html)\n",
    "    time.sleep(0.5) #Sleep for 0.5 seconds\n",
    "\n",
    "timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) #Local time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'lxml') #Make the BeautifulSoup object (soup): Take the HTML content as input and choose your parser (lxml)\n",
    "soup.select('.dre-hyphenate-text')[0].text #Selecting first title\n",
    "soup.find_all('div', class_ = 'dre-teaser-content')\n",
    "''.join(body_text)\n",
    "row_node.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "rows_list[0].contents\n",
    "row.append(child.text)\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# We open up google.com in a virtual browser\n",
    "url = 'https:google.com'\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install()) #Open virtual browser\n",
    "driver.get(url) #Go to google.com\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "cookie = driver.find_element(By.CSS_SELECTOR, '.cc-dismiss') #Here we use a CSS selector\n",
    "cookie.click()\n",
    "\n",
    "boardmember = driver.find_element(By.ID, 'mat-checkbox-2') #Here we use the id attribute to find the boardmember box\n",
    "boardmember.click()\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5): #We scroll down 5 times and sleep for 3 seconds each time to wait for the webpage to load\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") #Execute JavaScript on the browser that scroll down page\n",
    "    time.sleep(3)\n",
    "\n",
    "import re\n",
    "number_profiles = re.search(r'\\d+', profiles) #The '\\d' searches for digits in the string, and the '+' tells regex to search for all digits. The 'r' in the front of the string makes the string into a raw string; it means that for example \\n (new line) is not interpreted as new line, but is just seen as '\\n'.\n",
    "number_profiles = number_profiles.group()\n",
    "\n",
    "all_digits = re.findall(r'\\d+', text) \n",
    "sentences = re.split(r'\\.', text) #Remember that \".\" is a special character in regex, so we need to escape it with \"\\\"\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.implicitly_wait(100) #Waits maximum 100 seconds for an element to be found in the HTML.\n",
    "inputElement = driver.find_element(By.ID, 'query_domain')\n",
    "inputElement.send_keys('netbaby.dk')\n",
    "inputElement.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 4 # set default size of plots\n",
    "\n",
    "w = np.random.normal(0,0.001, size=(3)) # weight vector\n",
    "z = w[0] + X.dot(w[1:]) # (w[0]: bias, w[1:]: other weights, X: features)\n",
    "y_hat = np.where(positive, 1, -1)  # convert prediction\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=161193)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf.fit(X_train,y_train) # model training\n",
    "y_hat = clf.predict(X_test) # Use model to predict test target\n",
    "rgen = np.random.RandomState(seed = 161193)\n",
    "W = rgen.normal(loc = 0, scale = 0.01, size = X.shape[1]+1)  \n",
    "np.sign(net_input(X,W))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytrain, fitted_model.predict(Xtrain))\n",
    "stdev = np.std(xj)\n",
    "mean = np.mean(xj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,1,11)\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "for p in degrees:\n",
    "    X_train_p = PolynomialFeatures(degree=p).fit_transform(X_train) # poly. trans.\n",
    "    X_test_p = PolynomialFeatures(degree=p).fit_transform(X_test) # poly. trans.\n",
    "    \n",
    "    reg = LinearRegression().fit(X_train_p, y_train) # fit on train\n",
    "    \n",
    "    train_mse.append(mse(reg.predict(X_train_p),y_train)) # eval. performance on train\n",
    "    test_mse.append(mse(reg.predict(X_test_p),y_test))  # eval. performance on test     \n",
    "    parameters.append(reg.coef_) # store parameters\n",
    "\n",
    "degree_index = pd.Index(degrees,name='Polynomial degree ~ model complexity')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FROM SCIKIT-LEARN\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pipe = make_pipeline(PolynomialFeatures(degree = 2), \n",
    "                             StandardScaler(),\n",
    "                             LinearRegression()) #need to specify in correct order\n",
    "\n",
    "# Fit and evaluate model\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# LOAD DATA FROM SCIKIT-LEARN\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "\n",
    "# LOAD DATA FROM SCIKIT-LEARN\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# PREPARATION\n",
    "perform = [] # Store performance\n",
    "lambdas = np.logspace(-4, 4, 66) # Grid of lambdas\n",
    "\n",
    "# FOR EACH LAMBDA, FIT A MODEL ON TRAINING DATA, CHECK PERFORMANCE ON VALIDATION AND STORE MSE\n",
    "for lambda_ in lambdas:\n",
    "    pipe_lasso = make_pipeline(PolynomialFeatures(), \n",
    "                               StandardScaler(),\n",
    "                               Lasso(alpha=lambda_, random_state=161193))\n",
    "    pipe_lasso.fit(X_train, y_train)\n",
    "    y_pred = pipe_lasso.predict(X_val)\n",
    "    perform.append(mse(y_pred, y_val))\n",
    "\n",
    "# CRATE A SERIES WITH PERFORMANCE AND FIND PARM WITH LOWEST MSE    \n",
    "hyperparam_perform = pd.Series(perform,index=lambdas)\n",
    "optimal = hyperparam_perform.nsmallest(1)    \n",
    "print('Optimal lambda:', optimal.index[0])\n",
    "print('Validation MSE: %.3f' % optimal.values[0])\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfolds = KFold(n_splits=10)\n",
    "folds = list(kfolds.split(X_dev, y_dev))\n",
    "\n",
    "# Outer loop: lambdas\n",
    "mseCV = []\n",
    "for lambda_ in lambdas:    \n",
    "    \n",
    "    # Inner loop: folds\n",
    "    mseCV_ = []    \n",
    "    for train_idx, val_idx in folds:        \n",
    "        \n",
    "        # Train model and compute MSE on test fold\n",
    "        pipe_lassoCV = make_pipeline(PolynomialFeatures(degree=2, include_bias=True),\n",
    "                                     StandardScaler(),\n",
    "                                     Lasso(alpha=lambda_, random_state=161193))            \n",
    "        X_train, y_train = X_dev[train_idx], y_dev[train_idx]\n",
    "        X_val, y_val = X_dev[val_idx], y_dev[val_idx] \n",
    "        pipe_lassoCV.fit(X_train, y_train)        \n",
    "        mseCV_.append(mse(pipe_lassoCV.predict(X_val), y_val))    \n",
    "        \n",
    "    # Store result    \n",
    "    mseCV.append(mseCV_) \n",
    "    \n",
    "# Convert to DataFrame\n",
    "lambdaCV = pd.DataFrame(mseCV, index=lambdas)\n",
    "\n",
    "\n",
    "# CHOOSE OPTIMAL HYPERPARAMETERS (mean of MSE's across folds)\n",
    "optimal_lambda = lambdaCV.mean(axis=1).nsmallest(1)\n",
    "\n",
    "# RETRAIN/RE-ESTIMATE MODEL USING OPTIMAL HYPERPARAMETERS AND COMPARE PERFORMANCE\n",
    "pipe_lassoCV = make_pipeline(PolynomialFeatures(include_bias=False), \n",
    "                             StandardScaler(),\n",
    "                             Lasso(alpha=optimal_lambda.index[0], random_state=161193))\n",
    "\n",
    "pipe_lassoCV.fit(X_dev,y_dev) #fit optimal lambda to entire development set: likely to improve performance slightly since we use more oberservations\n",
    "\n",
    "models = {'Lasso': pipe_lasso, 'Lasso CV': pipe_lassoCV, 'LinReg': pipe_lr}\n",
    "for name, model in models.items():\n",
    "    score = mse(model.predict(X_test),y_test)\n",
    "    print(name, round(score, 2))\n",
    "\n",
    "\n",
    "# LOAD FROM SCIKIT-LEARN\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = \\\n",
    "    learning_curve(estimator=pipe_lasso,\n",
    "                   X=X_dev,\n",
    "                   y=y_dev,\n",
    "                   train_sizes=np.arange(0.05, 1.05, .05),\n",
    "                   scoring='neg_mean_squared_error',                 \n",
    "                   cv=10)\n",
    "    \n",
    "mse_ = pd.DataFrame({'Train':-train_scores.mean(axis=1),\n",
    "                     'Test':-test_scores.mean(axis=1)})\\\n",
    "        .set_index(pd.Index(train_sizes,name='sample size'))\n",
    "\n",
    "mse_.head(5)\n",
    "\n",
    "\n",
    "# LOAD FROM SCIKIT-LEARN\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# FIT AND EVALUATE FOR DIFFERENT LAMBDAS\n",
    "train_scores, test_scores = \\\n",
    "    validation_curve(estimator=pipe_lasso,\n",
    "                     X=X_dev,\n",
    "                     y=y_dev,\n",
    "                     param_name='lasso__alpha', #built-in name of hyperparameter\n",
    "                     param_range=lambdas, #values to consider\n",
    "                     scoring='neg_mean_squared_error',                 \n",
    "                     cv=10)\n",
    "\n",
    "# OBTAIN MSE FOR DIFFERENT LAMBDAS AND PRINT BEST\n",
    "mse_score = pd.DataFrame({'Train':-train_scores.mean(axis=1),\n",
    "                          'Validation':-test_scores.mean(axis=1),\n",
    "                          'lambda':lambdas})\\\n",
    "              .set_index('lambda')   \n",
    "print(mse_score.Validation.nsmallest(1))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "pipe_el = make_pipeline(PolynomialFeatures(include_bias=False), \n",
    "                        StandardScaler(),\n",
    "                        ElasticNet())\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_el, \n",
    "                  param_grid={'elasticnet__alpha':lambdas,\n",
    "                              'elasticnet__l1_ratio':np.linspace(0,1,20)}, \n",
    "                  scoring='neg_mean_squared_error', \n",
    "                  cv=10)\n",
    "\n",
    "models['ElasicNetCV'] = gs.fit(X_dev, y_dev)\n",
    "\n",
    "gs.best_params_ # extract hyperparameters\n",
    "gs.best_estimator_.steps[2][1].coef_ # extract coeffiecients from model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "lambdas_new = np.logspace(-4, 4, 100)\n",
    "\n",
    "gs = RandomizedSearchCV(estimator = pipe_lasso,\n",
    "                                param_distribution = [{\"lasso__alpha\": lambdas_new}], \n",
    "                                cv = 10, \n",
    "                                scoring = \"neg_mean_squared_error\",\n",
    "                                n_iter = 12)\n",
    "\n",
    "gs.fit(X_dev,y_dev)\n",
    "\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re_find_speakers = re.compile(r\"\"\"\n",
    "                                Kl.\\s\\d\\d:\\d\\d  #Time\n",
    "                                [^:]{0,50}      #Everything except colon, maximum 50 characters\n",
    "                                :               #Colon\n",
    "                                \"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "def cleaner(document):\n",
    "    document = document.lower() #To lower case\n",
    "    document = re.sub(r'<[^>]*>', ' ', document) #Remove HTML\n",
    "    document = re.sub(r'[^\\w\\s]','', document) #Remove non-alphanumeric characters\n",
    "    return document\n",
    "\n",
    "df['review'] = df['review'].apply(cleaner)\n",
    "\n",
    "# Split at whitespace with the split() method\n",
    "review_tokens = review_cleaned.split()\n",
    "\n",
    "# Tokenize \n",
    "import nltk\n",
    "review_tokens = nltk.tokenize.word_tokenize(review_cleaned)\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Delete stop words \n",
    "stop = stopwords.words('english')\n",
    "review_nostop = [i for i in review_tokens if i not in stop]\n",
    "\n",
    "# Stemming\n",
    "porter = nltk.PorterStemmer()\n",
    "review_stemmed = [porter.stem(i) for i in review_nostop]\n",
    "\n",
    "# Lemmatize the words with the WordNetLemmatizer\n",
    "nltk.download('omw-1.4') #Download OpenMultilingualWordnet\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "review_lemma = [wnl.lemmatize(i) for i in review_nostop]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer() #Store the class in 'count' to ease coding\n",
    "review_array = df['review'].values[0:2] #Take the first two reviews and store them in an array\n",
    "bag = count.fit_transform(review_array) #fit_transform takes an array as input and outputs the bag of words\n",
    "count_array = bag.toarray() #Make the bag to an array\n",
    "matrix = pd.DataFrame(data=count_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer() #Ease coding\n",
    "bag_tfidf = tfidf.fit_transform(bag) #Compute the tf-idf score from the bag of words from before ('bag')\n",
    "tfidf_array = bag_tfidf.toarray() #Make the bag to an array\n",
    "matrix_tfidf = pd.DataFrame(data=tfidf_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "x_train_bag = tfidf.fit_transform(x_train)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=0) #Text classifier\n",
    "lr.fit(x_train_bag,y_train)\n",
    "x_test_bag = tfidf.transform(x_test)\n",
    "train_preds = lr.predict(x_train_bag)\n",
    "test_preds = lr.predict(x_test_bag)\n",
    "\n",
    "features = ['_'.join(s.split()) for s in tfidf.get_feature_names_out()]\n",
    "coefficients = lr.coef_\n",
    "coefs_df = pd.DataFrame.from_records(coefficients, columns=features)\n",
    "\n",
    "\n",
    "# AFINN sentiment \n",
    "from afinn import Afinn\n",
    "\n",
    "afn = Afinn(emoticons=True) #Also use the emoticons in the lexicon\n",
    "review_sample=df.loc[[0,1000,49000]] #Choose some reviews from the cleaned dataset\n",
    "for i, row in review_sample.iterrows(): #Print the review, actual sentiment, and polarity score\n",
    "  print(\"REVIEW: \", row.review)\n",
    "  print(\"Actual Sentiment: \", row.sentiment)\n",
    "  print('Predicted Sentiment polarity: ', afn.score(row.review)) #Get the AFINN polarity score\n",
    "\n",
    "# VADER sentiment \n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "review_sample=df.loc[[0,1000,49000]] #Choose some reviews from the cleaned dataset\n",
    "for i, row in review_sample.iterrows(): #Print the review, actual sentiment, and polarity score\n",
    "  print(\"REVIEW: \", row.review)\n",
    "  print(\"Actual Sentiment: \", row.sentiment)\n",
    "  print('Predicted Sentiment polarity: ', analyser.polarity_scores(row.review)) #Get the VADER polarity score \n",
    "\n",
    "\n",
    "# LDA topic modelling \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,random_state=123) #The random_state parameter pass an integer that makes the result reproducible \n",
    "review_topics = lda.fit_transform(bag)\n",
    "n_top_words = 5\n",
    "word_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_): #lda.components_ stores a matrix containing the word importance for each topic\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([word_names[i]\n",
    "    for i in topic.argsort()\\\n",
    "        [:-n_top_words - 1:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a3d8ce103517c0eff4a4cdd3845d26e3393bafb2f4718f9c642b57a28f3097f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
