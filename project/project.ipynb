{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  imports and set magics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(response: requests.Response):\n",
    "    \"\"\"\n",
    "    Creates or appends a log-file with information from a requests.get()-call.\n",
    "    \n",
    "    The information gathered is:\n",
    "    - - - - - - - -\n",
    "        timestamp   :   Current local time.\n",
    "        status_code :   Status code from requests call.\n",
    "        length      :   Length of the HTML-string.\n",
    "        output_path :   Current working directory.\n",
    "        url         :   The URL of the response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile('log'):\n",
    "        log = open('log','a')\n",
    "    else: \n",
    "        log = open('log','w')\n",
    "        header = ['timestamp', 'status_code', 'length', 'output_file', 'url'] # Header names\n",
    "        log.write(';'.join(header) + \"\\n\")\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response.status_code # Status code from the request result\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) # Local time\n",
    "    length = len(response.text) # Length of the HTML-string\n",
    "    output_path = os.getcwd() # Output path\n",
    "    url = response.url # URL-string\n",
    "    \n",
    "    # Open the log file and append the gathered log information\n",
    "    with open('log','a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{output_path};{url}' + \"\\n\") \n",
    "\n",
    "\n",
    "def create_url(page: int) -> str:\n",
    "    \"\"\"\n",
    "    Creates a PolitiFact URL with the given pagenumber.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    page (int) :    Pagenumber for the PolitiFact website.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    url (str)  :    URL of the PolitiFact website for given page. \n",
    "    \"\"\"\n",
    "\n",
    "    url = f'https://www.politifact.com/factchecks/list/?page={page}' # Construct url with f-string\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_soup(url: str, header: dict) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Constructs a HTML-string from a request of the given URL. \n",
    "    Requests are logged, see log(). \n",
    "\n",
    "    Input:\n",
    "    - - - - - - - - \n",
    "    url (str)     :    URL of the website to receive the HTML-string from. \\n\n",
    "    header (dict) :    Dictionary to send in the query string for the request.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    soup (BeautifulSoup) :  HTML-string in the class of BeutifulSoup with 'lxml' parser.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url, headers=header) # Request\n",
    "    log(response) # Log \n",
    "    soup = BeautifulSoup(response.content, 'lxml') # Convert to response to HTML\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_articles(soup: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    Extracts articles from HTML-string from the PolitiFact website.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    soup (BeautifulSoup) : HTML-string from the PolitiFact website.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    list_of_articles (list) : A list of all articles in the given soup. \\n\n",
    "                              Each element is an article of data structure as BeautifulSoup.\n",
    "    \"\"\"\n",
    "    \n",
    "    articles = soup.find(class_='o-listicle__list') # Find section with articles\n",
    "    list_of_articles = articles.find_all('li') # Find all articles as a list\n",
    "\n",
    "    return list_of_articles\n",
    "\n",
    "\n",
    "def extract_info(article: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    Extracts all relevant information from an article on the PolitiFact website.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - - \n",
    "    article (BeautifulSoup) :  Article to extract data from, see extract_articles().\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    [name_txt, name_href, description_txt, quote_txt, quote_href, meter, footer] (list) \\n \n",
    "    The name and URL of the quoted person, the description of the quote, the quote itself \\n\n",
    "    and link hereof, the truthfulness index, and information on the article in string-format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Statement name \n",
    "    name = article.find(class_='m-statement__name')\n",
    "    name_txt = name.text # name \n",
    "    name_href = name['href'] # href\n",
    "\n",
    "    # Statement description\n",
    "    description_txt = article.find(class_='m-statement__desc').text\n",
    "\n",
    "    # Statement quote\n",
    "    quote = article.find(class_='m-statement__quote').a\n",
    "    quote_txt = quote.text # name \n",
    "    quote_href = quote['href'] # href\n",
    "\n",
    "    # Statement meter\n",
    "    meter = article.find(class_='m-statement__meter').div.img['alt']\n",
    "\n",
    "    # Statement footer\n",
    "    footer = article.find(class_='m-statement__footer').text\n",
    "\n",
    "    return [name_txt, name_href, description_txt, quote_txt, quote_href, meter, footer]\n",
    "\n",
    "\n",
    "def data_politifact(startpage: int, endpage: int, header: dict) -> list:\n",
    "    \"\"\"\n",
    "    Compound function that scrapes an interval of pages from PolitiFact and extracts information for analysis. \\n\n",
    "    Saves extracted information for each page in '/data'-folder as CSV, and logs requests in 'log'. \n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    startpage (int) :  The first page to scrape. \\n\n",
    "    endpage   (int) :  The last page to scrape. \\n\n",
    "    header    (dict):  Dictionary to send in the query string for the request.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    list_of_dfs (list) : A list of pandas.DataFrame containing the extracted information from each page.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_dfs = [] # initialize empty list for dataframes\n",
    "\n",
    "    # Loop through pages and track progress with tqdm\n",
    "    for page in tqdm.tqdm(range(startpage, endpage+1)):\n",
    "        url = create_url(page) # create url\n",
    "\n",
    "        try: # circumvent problem with empty pages\n",
    "            soup = get_soup(url, header) # construct html\n",
    "            articles = extract_articles(soup) # extract articles \n",
    "\n",
    "            output = [] # initialize empty for articles \n",
    "\n",
    "            # Loop through articles \n",
    "            for article in articles:\n",
    "                info = extract_info(article) # extract relevant information\n",
    "                output.append(info) # append output\n",
    "\n",
    "        except: # skip page\n",
    "            continue\n",
    "\n",
    "        # Create DataFrame\n",
    "        output_df = pd.DataFrame(output, columns=['name_txt', 'name_href', 'description_txt', 'quote_txt', 'quote_href', 'meter', 'footer'])\n",
    "\n",
    "        # Create data-folder if it doesn't exist\n",
    "        path = os.getcwd() + '/data/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # Save CSV-file and append list of DataFrames\n",
    "        output_df.to_csv(path + f'data_p{page}', index=False) # save csv\n",
    "        list_of_dfs.append(output_df) # append df\n",
    "\n",
    "        \n",
    "\n",
    "        time.sleep(0.5) # sleep for 0.5 sec \n",
    "\n",
    "    return list_of_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape all pages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do *one* of the following three:\n",
    "1. Download all data. **NB!** Takes ~30 minutes.\n",
    "2. Load data from data folder if data has been downloaded.\n",
    "3. Load full dataset if data has been downloaded and concatenated.\n",
    "\n",
    "Option 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:06<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "header = {  'name_1':'Marius Heltberg Lassen'   ,'email_1':'pgb206@alumni.ku.dk', \n",
    "            'name_2':'Jørgen Baun Høst'         ,'email_2':'pjz633@alumni.ku.dk',\n",
    "            'intention':'Academic purpose: scrape politifact.com to train supervised ML model for fake news detection' } # state names and (non-commerical/academic) intentions for data scraping\n",
    "#dfs = data_politifact(1, 728, header)\n",
    "data_full = pd.concat(dfs)\n",
    "data_full.to_csv('data_full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in os.listdir('data'):\n",
    "    dfs.append(pd.read_csv('data/' + file))\n",
    "data_full = pd.concat(dfs)\n",
    "data_full.to_csv('data_full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('data_full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "054e95b4819972eba8d406807e822e3be9cca805528e86310f8e3ac8dc287778"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
