{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  imports and set magics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(response: requests.Response):\n",
    "    \"\"\"\n",
    "    Creates or appends a log-file with information from a requests.get()-call.\n",
    "    \n",
    "    The information gathered is:\n",
    "    - - - - - - - -\n",
    "        timestamp   :   Current local time.\n",
    "        status_code :   Status code from requests call.\n",
    "        length      :   Length of the HTML-string.\n",
    "        output_path :   Current working directory.\n",
    "        url         :   The URL of the response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile('log'):\n",
    "        log = open('log','a')\n",
    "    else: \n",
    "        log = open('log','w')\n",
    "        header = ['timestamp', 'status_code', 'length', 'output_file', 'url'] # Header names\n",
    "        log.write(';'.join(header) + \"\\n\")\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response.status_code # Status code from the request result\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) # Local time\n",
    "    length = len(response.text) # Length of the HTML-string\n",
    "    output_path = os.getcwd() # Output path\n",
    "    url = response.url # URL-string\n",
    "    \n",
    "    # Open the log file and append the gathered log information\n",
    "    with open('log','a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{output_path};{url}' + \"\\n\") \n",
    "\n",
    "\n",
    "def create_url(page: int) -> str:\n",
    "    \"\"\"\n",
    "    Creates a PolitiFact URL with the given pagenumber.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    page (int) :    Pagenumber for the PolitiFact website.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    url (str)  :    URL of the PolitiFact website for given page. \n",
    "    \"\"\"\n",
    "\n",
    "    url = f'https://www.politifact.com/factchecks/list/?page={page}' # Construct url with f-string\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_soup(url: str, header: dict) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Constructs a HTML-string from a request of the given URL. \n",
    "    Requests are logged, see log(). \n",
    "\n",
    "    Input:\n",
    "    - - - - - - - - \n",
    "    url (str)     :    URL of the website to receive the HTML-string from. \\n\n",
    "    header (dict) :    Dictionary to send in the query string for the request.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    soup (BeautifulSoup) :  HTML-string in the class of BeutifulSoup with 'lxml' parser.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url, headers=header) # Request\n",
    "    log(response) # Log \n",
    "    soup = BeautifulSoup(response.content, 'lxml') # Convert to response to HTML\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_articles(soup: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    Extracts articles from HTML-string from the PolitiFact website.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    soup (BeautifulSoup) : HTML-string from the PolitiFact website.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    list_of_articles (list) : A list of all articles in the given soup. \\n\n",
    "                              Each element is an article of data structure as BeautifulSoup.\n",
    "    \"\"\"\n",
    "    \n",
    "    articles = soup.find(class_='o-listicle__list') # Find section with articles\n",
    "    list_of_articles = articles.find_all('li') # Find all articles as a list\n",
    "\n",
    "    return list_of_articles\n",
    "\n",
    "\n",
    "def extract_info(article: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    Extracts all relevant information from an article on the PolitiFact website.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - - \n",
    "    article (BeautifulSoup) :  Article to extract data from, see extract_articles().\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    [name_txt, name_href, description_txt, quote_txt, quote_href, meter, footer] (list) \\n \n",
    "    The name and URL of the quoted person, the description of the quote, the quote itself \\n\n",
    "    and link hereof, the truthfulness index, and information on the article in string-format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Statement name \n",
    "    name = article.find(class_='m-statement__name')\n",
    "    name_txt = name.text # name \n",
    "    name_href = name['href'] # href\n",
    "\n",
    "    # Statement description\n",
    "    description_txt = article.find(class_='m-statement__desc').text\n",
    "\n",
    "    # Statement quote\n",
    "    quote = article.find(class_='m-statement__quote').a\n",
    "    quote_txt = quote.text # name \n",
    "    quote_href = quote['href'] # href\n",
    "\n",
    "    # Statement meter\n",
    "    meter = article.find(class_='m-statement__meter').div.img['alt']\n",
    "\n",
    "    # Statement footer\n",
    "    footer = article.find(class_='m-statement__footer').text\n",
    "\n",
    "    return [name_txt, name_href, description_txt, quote_txt, quote_href, meter, footer]\n",
    "\n",
    "\n",
    "def data_politifact(startpage: int, endpage: int, header: dict) -> list:\n",
    "    \"\"\"\n",
    "    Compound function that scrapes an interval of pages from PolitiFact and extracts information for analysis. \\n\n",
    "    Saves extracted information for each page in '/data'-folder as CSV, and logs requests in 'log'. \n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    startpage (int) :  The first page to scrape. \\n\n",
    "    endpage   (int) :  The last page to scrape. \\n\n",
    "    header    (dict):  Dictionary to send in the query string for the request.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    list_of_dfs (list) : A list of pandas.DataFrame containing the extracted information from each page.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_dfs = [] # initialize empty list for dataframes\n",
    "\n",
    "    # Loop through pages and track progress with tqdm\n",
    "    for page in tqdm.tqdm(range(startpage, endpage+1)):\n",
    "        url = create_url(page) # create url\n",
    "\n",
    "        try: # circumvent problem with empty pages\n",
    "            soup = get_soup(url, header) # construct html\n",
    "            articles = extract_articles(soup) # extract articles \n",
    "\n",
    "            output = [] # initialize empty for articles \n",
    "\n",
    "            # Loop through articles \n",
    "            for article in articles:\n",
    "                info = extract_info(article) # extract relevant information\n",
    "                output.append(info) # append output\n",
    "\n",
    "        except: # skip page\n",
    "            continue\n",
    "\n",
    "        # Create DataFrame\n",
    "        output_df = pd.DataFrame(output, columns=['name_txt', 'name_href', 'description_txt', 'quote_txt', 'quote_href', 'meter', 'footer'])\n",
    "\n",
    "        # Create data-folder if it doesn't exist\n",
    "        path = os.getcwd() + '/data/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # Save CSV-file and append list of DataFrames\n",
    "        output_df.to_csv(path + f'data_p{page}', index=False) # save csv\n",
    "        list_of_dfs.append(output_df) # append df\n",
    "\n",
    "        \n",
    "\n",
    "        time.sleep(0.5) # sleep for 0.5 sec \n",
    "\n",
    "    return list_of_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape all pages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do *one* of the following three:\n",
    "1. Download all data. **NB!** Takes ~30 minutes.\n",
    "2. Load data from data folder if data has been downloaded.\n",
    "3. Load full dataset if data has been downloaded and concatenated.\n",
    "\n",
    "Option 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {  'name_1':'Marius Heltberg Lassen'   ,'email_1':'pgb206@alumni.ku.dk', \n",
    "            'name_2':'Jørgen Baun Høst'         ,'email_2':'pjz633@alumni.ku.dk',\n",
    "            'intention':'Train supervised ML model for academic purposes' } # state names and (non-commerical/academic) intentions for data scraping\n",
    "#dfs = data_politifact(1, 728, header)\n",
    "data_full = pd.concat(dfs)\n",
    "data_full.to_csv('data_full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in os.listdir('data'):\n",
    "    dfs.append(pd.read_csv('data/' + file))\n",
    "data_full = pd.concat(dfs)\n",
    "data_full.to_csv('data_full', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = pd.read_csv('data_full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_txt</th>\n",
       "      <th>name_href</th>\n",
       "      <th>description_txt</th>\n",
       "      <th>quote_txt</th>\n",
       "      <th>quote_href</th>\n",
       "      <th>meter</th>\n",
       "      <th>footer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nRobert Hurt\\n</td>\n",
       "      <td>/personalities/robert-hurt/</td>\n",
       "      <td>\\nstated on April 16, 2015 in a statement.:\\n</td>\n",
       "      <td>\\nSays the estate tax, \"in many cases,\" forces...</td>\n",
       "      <td>/factchecks/2015/may/03/robert-hurt/hurt-amiss...</td>\n",
       "      <td>false</td>\n",
       "      <td>\\nBy Warren Fiske • May 3, 2015\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nMarco Rubio\\n</td>\n",
       "      <td>/personalities/marco-rubio/</td>\n",
       "      <td>\\nstated on April 13, 2015 in an interview on ...</td>\n",
       "      <td>\\n\"The Iranians are now saying that what we're...</td>\n",
       "      <td>/factchecks/2015/may/01/marco-rubio/iran-unite...</td>\n",
       "      <td>true</td>\n",
       "      <td>\\nBy Lauren Carroll • May 1, 2015\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nCity of Atlanta\\n</td>\n",
       "      <td>/personalities/city-atlanta/</td>\n",
       "      <td>\\nstated on August 8, 2014 in press release:\\n</td>\n",
       "      <td>\\nTyler Perry’s plan to turn a majority of the...</td>\n",
       "      <td>/factchecks/2015/may/01/city-atlanta/Studio-pl...</td>\n",
       "      <td>half-true</td>\n",
       "      <td>\\nBy Nancy Badertscher • May 1, 2015\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nRepresent.us\\n</td>\n",
       "      <td>/personalities/representus/</td>\n",
       "      <td>\\nstated on April 30, 2015 in a meme on social...</td>\n",
       "      <td>\\n\"The U.S. representatives that voted to keep...</td>\n",
       "      <td>/factchecks/2015/apr/30/representus/did-lawmak...</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>\\nBy Louis Jacobson • April 30, 2015\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nSteve Crisafulli\\n</td>\n",
       "      <td>/personalities/steve-crisafulli/</td>\n",
       "      <td>\\nstated on April 28, 2015 in an op-ed in the ...</td>\n",
       "      <td>\\n\"If we choose Obamacare expansion, 600,000 w...</td>\n",
       "      <td>/factchecks/2015/apr/30/steve-crisafulli/crisa...</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>\\nBy Joshua Gillin • April 30, 2015\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name_txt                         name_href  \\\n",
       "0       \\nRobert Hurt\\n       /personalities/robert-hurt/   \n",
       "1       \\nMarco Rubio\\n       /personalities/marco-rubio/   \n",
       "2   \\nCity of Atlanta\\n      /personalities/city-atlanta/   \n",
       "3      \\nRepresent.us\\n       /personalities/representus/   \n",
       "4  \\nSteve Crisafulli\\n  /personalities/steve-crisafulli/   \n",
       "\n",
       "                                     description_txt  \\\n",
       "0      \\nstated on April 16, 2015 in a statement.:\\n   \n",
       "1  \\nstated on April 13, 2015 in an interview on ...   \n",
       "2     \\nstated on August 8, 2014 in press release:\\n   \n",
       "3  \\nstated on April 30, 2015 in a meme on social...   \n",
       "4  \\nstated on April 28, 2015 in an op-ed in the ...   \n",
       "\n",
       "                                           quote_txt  \\\n",
       "0  \\nSays the estate tax, \"in many cases,\" forces...   \n",
       "1  \\n\"The Iranians are now saying that what we're...   \n",
       "2  \\nTyler Perry’s plan to turn a majority of the...   \n",
       "3  \\n\"The U.S. representatives that voted to keep...   \n",
       "4  \\n\"If we choose Obamacare expansion, 600,000 w...   \n",
       "\n",
       "                                          quote_href        meter  \\\n",
       "0  /factchecks/2015/may/03/robert-hurt/hurt-amiss...        false   \n",
       "1  /factchecks/2015/may/01/marco-rubio/iran-unite...         true   \n",
       "2  /factchecks/2015/may/01/city-atlanta/Studio-pl...    half-true   \n",
       "3  /factchecks/2015/apr/30/representus/did-lawmak...  mostly-true   \n",
       "4  /factchecks/2015/apr/30/steve-crisafulli/crisa...  mostly-true   \n",
       "\n",
       "                                   footer  \n",
       "0       \\nBy Warren Fiske • May 3, 2015\\n  \n",
       "1     \\nBy Lauren Carroll • May 1, 2015\\n  \n",
       "2  \\nBy Nancy Badertscher • May 1, 2015\\n  \n",
       "3  \\nBy Louis Jacobson • April 30, 2015\\n  \n",
       "4   \\nBy Joshua Gillin • April 30, 2015\\n  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = {  'name_1':'Marius Heltberg Lassen'   ,'email_1':'pgb206@alumni.ku.dk', \n",
    "            'name_2':'Jørgen Baun Høst'         ,'email_2':'pjz633@alumni.ku.dk',\n",
    "            'intention':'Train supervised ML model for academic purposes' } # state names and (non-commerical/academic) intentions for data scraping\n",
    "data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define article data extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_data(article: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    A function that scrapes each individual article for relevant data. \\n\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    article (BeautifulSoup) : BeatifulSoup element of article. \\n\n",
    "    \n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    [tags,sub_header,text_body,sources, quote_href] : A list of of all relevant data from each politifact article.\n",
    "\n",
    "    \"\"\"\n",
    "    base_url = 'https://www.politifact.com'\n",
    "    quote_href = article.find('meta', property='og:url')['content']\n",
    "    quote_href = quote_href.replace(base_url, '') #Extract the quote_href from the meta data\n",
    "\n",
    "    tag_soup = article.find(class_='m-list m-list--horizontal')\\\n",
    "        .find_all('a') #Find all tags\n",
    "    \n",
    "    list_of_tags = [] #Create empty list for tags \n",
    "    \n",
    "    for tag in tag_soup:\n",
    "        list_of_tags.append(tag['title']) #Append each tag to list of tags\n",
    "\n",
    "    sub_header = article.find(class_='c-title c-title--subline').text #conclusion by journalist\n",
    "\n",
    "    text_block = article.find(class_='m-textblock') #Find article's body text\n",
    "    text_body = []\n",
    "\n",
    "    for paragraph in text_block.find_all('p'): #Find all paragraphs in article\n",
    "        text_body.append(paragraph.text) #append them to list\n",
    "\n",
    "    text_body=' '.join(text_body) #Convert to a single string\n",
    "\n",
    "    source_block = article.find(class_='m-superbox__content')\\\n",
    "                    .find_all('p') #Find article's source block and paragraphs\n",
    "    \n",
    "    source_body = []\n",
    "    source_link = []\n",
    "\n",
    "    for paragraph in source_block:\n",
    "        source_body.append(paragraph.text) #Find text in source paragraph and append\n",
    "\n",
    "    for paragraph in source_block:\n",
    "        try:\n",
    "            source_link.append(paragraph.a['href']) #append link if it's there\n",
    "        except:\n",
    "            continue\n",
    "        source_link.append('No link') #Append 'no link' if there's no url. \n",
    "                                        #Is this how we wanna do it??\n",
    "            \n",
    "    sources = list(zip(source_body,source_link))\n",
    "\n",
    "    return [list_of_tags, sub_header,text_body,sources, quote_href]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_articles(list_of_url: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A compound function that scrapes relevant data from each article on politifact.com and stores this in a DataFrame. \\n\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    list_of_url (list) : A list of URL's for each article to scrape. \\n\n",
    "    \n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    list_of_dfs (list) : A list of of dataframe for each article.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for article_url in tqdm.tqdm(list_of_url): \n",
    "        article = get_soup(article_url, header=header) #Get BeautifulSoup element for each article\n",
    "        article_data = get_article_data(article) #Extract data from article\n",
    "        output.append(article_data) #Append data to output list\n",
    "    \n",
    "        output_df = pd.DataFrame(data=output, columns=['tags', 'sub_header', 'text_body', 'sources', 'quote_href']) #Convert to DataFrame\n",
    "\n",
    "\n",
    "    return output_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's give it a spin for the first 10 articles\n",
    "article_url_list = []\n",
    "url_base = 'https://politifact.com'\n",
    "\n",
    "for quote_href in data_full['quote_href']:\n",
    "    article_url_list.append(url_base+quote_href)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunked_list = []\n",
    "chunk_size = 30\n",
    "for i in range(0, len(article_url_list), chunk_size):\n",
    "    chunked_list.append(article_url_list[i:i+chunk_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:04<00:00,  7.03it/s]\n",
      "100%|██████████| 30/30 [00:04<00:00,  6.42it/s]\n",
      "100%|██████████| 30/30 [00:03<00:00,  7.64it/s]\n",
      "100%|██████████| 30/30 [00:04<00:00,  7.38it/s]\n",
      "100%|██████████| 30/30 [00:04<00:00,  6.67it/s]\n",
      "100%|██████████| 30/30 [00:03<00:00,  7.65it/s]\n",
      "100%|██████████| 30/30 [00:40<00:00,  1.35s/it]\n",
      "100%|██████████| 30/30 [00:45<00:00,  1.53s/it]\n",
      "100%|██████████| 30/30 [00:41<00:00,  1.40s/it]\n",
      " 43%|████▎     | 13/30 [00:15<00:21,  1.25s/it]"
     ]
    }
   ],
   "source": [
    "path = os.getcwd() + '/article_data/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "stopped_at_loop_no = 580 #If the loop has stopped at number x\n",
    "#Problems with 569-570\n",
    "#Somehow made an error with the zip function --> take a look 'sources' from .csv 580-ish and onwards \n",
    "\n",
    "for it, list_no in enumerate(chunked_list[stopped_at_loop_no:]):\n",
    "    df = pd.DataFrame(get_all_articles(list_no))\n",
    "    df.to_csv(path + f'article_data_{it+stopped_at_loop_no}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4764\\1242427199.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_no\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunked_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlist_no\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0mlist_no\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_all_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_no\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mf'article_data_{it}_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "for it, list_no in enumerate(chunked_list[:5]):\n",
    "    if list_no == 2 | list_no ==3:\n",
    "        continue\n",
    "    df = pd.DataFrame(get_all_articles(list_no))\n",
    "    df.to_csv(path + f'article_data_{it}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it, list_no in enumerate(chunked_list):\n",
    "    if list_no == 569 | list_no ==570:\n",
    "        next(list_no)\n",
    "    df = pd.DataFrame(get_all_articles(list_no))\n",
    "    df.to_csv(path + f'article_data_{it}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word processing pseudo code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4') #Download OpenMultilingualWordnet\n",
    "\n",
    "\n",
    "review_tokens = nltk.tokenize.word_tokenize(review_cleaned) #Tokenize = splitting the doc/text into meaningful elements (tokens)\n",
    "review_tokens\n",
    "\n",
    "stop = stopwords.words('english') \n",
    "review_nostop = [i for i in review_tokens if i not in stop] #Remove commong stop words\n",
    "review_nostop\n",
    "\n",
    "print(len(review_tokens)) #How many tokens?\n",
    "print(len(review_nostop)) #How many stop words are removed?\n",
    "\n",
    "# Stem the words -> Transforming word into its root form\n",
    "porter = nltk.PorterStemmer()\n",
    "review_stemmed = [porter.stem(i) for i in review_nostop]\n",
    "review_stemmed\n",
    "\n",
    "\n",
    "#Bag of words -> Counts the number of times each words occurs in a doc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer() #Store the class in 'count' to ease coding\n",
    "review_array = df['review'].values[0:2] #Take the first two reviews and store them in an array\n",
    "bag = count.fit_transform(review_array) #fit_transform takes an array as input and outputs the bag of words\n",
    "count_array = bag.toarray() #Make the bag to an array\n",
    "matrix = pd.DataFrame(data=count_array,columns = count.get_feature_names_out()) #Input the bag and the words into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic modelling\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,random_state=123) #The random_state parameter pass an integer that makes the result reproducible \n",
    "review_topics = lda.fit_transform(bag)\n",
    "\n",
    "n_top_words = 5\n",
    "word_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_): #lda.components_ stores a matrix containing the word importance for each topic\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([word_names[i]\n",
    "    for i in topic.argsort()\\\n",
    "        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative to stemming: Find the grammatically correct form of the word \n",
    "#NB! Computationally intensive\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "review_lemma = [wnl.lemmatize(i) for i in review_nostop]\n",
    "review_lemma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "054e95b4819972eba8d406807e822e3be9cca805528e86310f8e3ac8dc287778"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
