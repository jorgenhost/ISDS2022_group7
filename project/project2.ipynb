{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  imports and set magics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(response: requests.Response):\n",
    "    \"\"\"\n",
    "    Creates or appends a log-file with information from a requests.get()-call.\n",
    "    \n",
    "    The information gathered is:\n",
    "    - - - - - - - -\n",
    "        timestamp   :   Current local time.\n",
    "        status_code :   Status code from requests call.\n",
    "        length      :   Length of the HTML-string.\n",
    "        output_path :   Current working directory.\n",
    "        url         :   The URL of the response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile('log'):\n",
    "        log = open('log','a')\n",
    "    else: \n",
    "        log = open('log','w')\n",
    "        header = ['timestamp', 'status_code', 'length', 'output_file', 'url'] # Header names\n",
    "        log.write(';'.join(header) + \"\\n\")\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response.status_code # Status code from the request result\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) # Local time\n",
    "    length = len(response.text) # Length of the HTML-string\n",
    "    output_path = os.getcwd() # Output path\n",
    "    url = response.url # URL-string\n",
    "    \n",
    "    # Open the log file and append the gathered log information\n",
    "    with open('log','a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{output_path};{url}' + \"\\n\") \n",
    "\n",
    "\n",
    "def create_url(page: int) -> str:\n",
    "    \"\"\"\n",
    "    Creates a PolitiFact URL with the given pagenumber.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    page (int) :    Pagenumber for the PolitiFact website.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    url (str)  :    URL of the PolitiFact website for given page. \n",
    "    \"\"\"\n",
    "\n",
    "    url = f'https://www.politifact.com/factchecks/list/?page={page}' # Construct url with f-string\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_soup(url: str, header: dict) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Constructs a HTML-string from a request of the given URL. \n",
    "    Requests are logged, see log(). \n",
    "\n",
    "    Input:\n",
    "    - - - - - - - - \n",
    "    url (str)     :    URL of the website to receive the HTML-string from. \\n\n",
    "    header (dict) :    Dictionary to send in the query string for the request.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    soup (BeautifulSoup) :  HTML-string in the class of BeutifulSoup with 'lxml' parser.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url, headers=header) # Request\n",
    "    log(response) # Log \n",
    "    soup = BeautifulSoup(response.content, 'lxml') # Convert to response to HTML\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_articles(soup: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    Extracts articles from HTML-string from the PolitiFact website.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    soup (BeautifulSoup) : HTML-string from the PolitiFact website.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    list_of_articles (list) : A list of all articles in the given soup. \\n\n",
    "                              Each element is an article of data structure as BeautifulSoup.\n",
    "    \"\"\"\n",
    "    \n",
    "    articles = soup.find(class_='o-listicle__list') # Find section with articles\n",
    "    list_of_articles = articles.find_all('li') # Find all articles as a list\n",
    "\n",
    "    return list_of_articles\n",
    "\n",
    "\n",
    "def extract_info(article: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    Extracts all relevant information from an article on the PolitiFact website.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - - \n",
    "    article (BeautifulSoup) :  Article to extract data from, see extract_articles().\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    [name_txt, name_href, description_txt, quote_txt, quote_href, meter, footer] (list) \\n \n",
    "    The name and URL of the quoted person, the description of the quote, the quote itself \\n\n",
    "    and link hereof, the truthfulness index, and information on the article in string-format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Statement name \n",
    "    name = article.find(class_='m-statement__name')\n",
    "    name_txt = name.text # name \n",
    "    name_href = name['href'] # href\n",
    "\n",
    "    # Statement description\n",
    "    description_txt = article.find(class_='m-statement__desc').text\n",
    "\n",
    "    # Statement quote\n",
    "    quote = article.find(class_='m-statement__quote').a\n",
    "    quote_txt = quote.text # name \n",
    "    quote_href = quote['href'] # href\n",
    "\n",
    "    # Statement meter\n",
    "    meter = article.find(class_='m-statement__meter').div.img['alt']\n",
    "\n",
    "    # Statement footer\n",
    "    footer = article.find(class_='m-statement__footer').text\n",
    "\n",
    "    return [name_txt, name_href, description_txt, quote_txt, quote_href, meter, footer]\n",
    "\n",
    "\n",
    "def data_politifact(startpage: int, endpage: int, header: dict) -> list:\n",
    "    \"\"\"\n",
    "    Compound function that scrapes an interval of pages from PolitiFact and extracts information for analysis. \\n\n",
    "    Saves extracted information for each page in '/data'-folder as CSV, and logs requests in 'log'. \n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    startpage (int) :  The first page to scrape. \\n\n",
    "    endpage   (int) :  The last page to scrape. \\n\n",
    "    header    (dict):  Dictionary to send in the query string for the request.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    list_of_dfs (list) : A list of pandas.DataFrame containing the extracted information from each page.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_dfs = [] # initialize empty list for dataframes\n",
    "\n",
    "    # Loop through pages and track progress with tqdm\n",
    "    for page in tqdm.tqdm(range(startpage, endpage+1)):\n",
    "        url = create_url(page) # create url\n",
    "\n",
    "        try: # circumvent problem with empty pages\n",
    "            soup = get_soup(url, header) # construct html\n",
    "            articles = extract_articles(soup) # extract articles \n",
    "\n",
    "            output = [] # initialize empty for articles \n",
    "\n",
    "            # Loop through articles \n",
    "            for article in articles:\n",
    "                info = extract_info(article) # extract relevant information\n",
    "                output.append(info) # append output\n",
    "\n",
    "        except: # skip page\n",
    "            continue\n",
    "\n",
    "        # Create DataFrame\n",
    "        output_df = pd.DataFrame(output, columns=['name_txt', 'name_href', 'description_txt', 'quote_txt', 'quote_href', 'meter', 'footer'])\n",
    "\n",
    "        # Create data-folder if it doesn't exist\n",
    "        path = os.getcwd() + '/page_data/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # Save CSV-file and append list of DataFrames\n",
    "        output_df.to_parquet(path + f'data_p{page}.pq') # save parquet file (this keeps datatypes)\n",
    "        list_of_dfs.append(output_df) # append df\n",
    "\n",
    "        \n",
    "        time.sleep(0.5) # sleep for 0.5 sec \n",
    "\n",
    "    return list_of_dfs\n",
    "\n",
    "\n",
    "def get_article_data(article: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    A function that scrapes each individual article for relevant data. \\n\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    article (BeautifulSoup) : BeatifulSoup element of article. \\n\n",
    "    \n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    [tags, sub_header, text_body, quote_href] : A list of of all relevant data from each politifact article.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract tags\n",
    "    tag_soup = article.find(class_='m-list m-list--horizontal')\\\n",
    "        .find_all('a') #Find all tags\n",
    "    \n",
    "    list_of_tags = [] #Create empty list for tags \n",
    "    \n",
    "    for tag in tag_soup:\n",
    "        list_of_tags.append(tag['title']) #Append each tag to list of tags\n",
    "\n",
    "    # Extract sub-header\n",
    "    sub_header = article.find(class_='c-title c-title--subline').text #conclusion by journalist\n",
    "\n",
    "    # Extract entire text body\n",
    "    text_block = article.find(class_='m-textblock') #Find article's body text\n",
    "    text_body = []\n",
    "\n",
    "    for paragraph in text_block.find_all('p'): #Find all paragraphs in article\n",
    "        text_body.append(paragraph.text) #append them to list\n",
    "\n",
    "    text_body=' '.join(text_body) #Convert to a single string\n",
    "\n",
    "    source_block = article.find(class_='m-superbox__content')\\\n",
    "                    .find_all('p') #Find article's source block and paragraphs\n",
    "\n",
    "    source_body = []\n",
    "    source_link = []\n",
    "\n",
    "    for paragraph in source_block:\n",
    "        source_body.append(paragraph.text) #Find text in source paragraph and append\n",
    "\n",
    "    for paragraph in source_block:\n",
    "        try:\n",
    "            source_link.append(paragraph.a['href']) #append link if it's there\n",
    "        except:\n",
    "            continue\n",
    "        source_link.append('No link') #Append 'no link' if there's no url. \n",
    "\n",
    "\n",
    "    sources = [x for x in zip(source_body, source_link)] #Store  sources in list of tuples (the sources 'text' and, if applicable, the link itself)\n",
    "\n",
    "    return [list_of_tags, sub_header, text_body, sources]\n",
    "\n",
    "\n",
    "def get_all_articles(list_of_url: list, header: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A compound function that scrapes relevant data from each article on politifact.com and stores this in a DataFrame. \\n\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    list_of_url (list) : A list of URL's for each article to scrape. \\n\n",
    "    header      (dict) : Dictionary to send in the query string for the request.\n",
    "    \n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    list_of_dfs (list) : A list of of dataframe for each article.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Split list_of_url into chunks of 30 URLs\n",
    "    chunked_list = []\n",
    "    for i in range(0, len(list_of_url), 30):\n",
    "        chunked_list.append(list_of_url[i:i+30])\n",
    "\n",
    "    # Create data-folder if it doesn't exist\n",
    "    path = os.getcwd() + '/article_data/'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    list_of_dfs = [] # Initialize empty list for dataframes\n",
    "\n",
    "    for it, chunk in tqdm.tqdm(enumerate(chunked_list)):\n",
    "        output = [] # Initialize empty output list \n",
    "\n",
    "        # Loop through list of URls\n",
    "        for article_url in chunk: \n",
    "            full_url = 'https://www.politifact.com' + article_url\n",
    "            article = get_soup(full_url, header=header) #Get BeautifulSoup element for each article\n",
    "            try:    \n",
    "                article_data = get_article_data(article) #Extract data from article\n",
    "            except: \n",
    "                continue\n",
    "            article_data.append(article_url) #Quote_href\n",
    "            output.append(article_data) #Append data to output list\n",
    "            time.sleep(0.5) #Sleep for 0.5 seconds\n",
    "\n",
    "        # Create DataFrame\n",
    "        output_df = pd.DataFrame(data=output, columns=['tags', 'sub_header', 'text_body', 'sources', 'quote_href']) #Convert to DataFrame\n",
    "        \n",
    "        # Save pq-file and append list of DataFrames\n",
    "        name_it = it\n",
    "        filename = f'article_data_{name_it}.pq'\n",
    "        while os.path.exists(path + filename):\n",
    "            name_it += 1\n",
    "            filename = f'article_data_{name_it}.pq'\n",
    "        print(filename)\n",
    "        output_df.to_parquet(path + filename)\n",
    "        list_of_dfs.append(output_df)\n",
    "\n",
    "    return list_of_dfs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape all pages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code will do *one* of the following three:\n",
    "1. Load full dataset if data has been downloaded and concatenated.\n",
    "2. Load data from data folder if data has been downloaded. (Note: You will be asked to delete the data folder or dowload missing files manually, if all data has not already been downloaded and saved in the folder.)\n",
    "3. Download all data. **NB!** Takes ~30 minutes.\n",
    "\n",
    "When the code has been run, the dataset `data_full`, will contain raw data with summary information for all articles on [PolitiFact](https://www.politifact.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {  'name_1':'Marius Heltberg Lassen'   ,'email_1':'pgb206@alumni.ku.dk', \n",
    "            'name_2':'Jørgen Baun Høst'         ,'email_2':'pjz633@alumni.ku.dk',\n",
    "            'intention':'Train supervised ML model for academic purposes' } # state names and (non-commerical/academic) intentions for data scraping\n",
    "\n",
    "path = os.getcwd()\n",
    "if os.path.exists('page_data_merged.pq'): \n",
    "    page_data_merged = pd.read_parquet('page_data_merged.pq')\n",
    "elif os.path.exists('page_data'):\n",
    "    assert len(os.listdir('page_data')) >= 722, \"Delete folder 'page_data', or download missing files manually\"\n",
    "    dfs = []\n",
    "    for file in os.listdir('page_data'):\n",
    "        dfs.append(pd.read_parquet('page_data/' + file))\n",
    "    page_data_merged = pd.concat(dfs)\n",
    "    page_data_merged.to_parquet('page_data_merged.pq')\n",
    "else: \n",
    "    dfs = data_politifact(1, 728, header)\n",
    "    page_data_merged = pd.concat(dfs)\n",
    "    page_data_merged.to_parquet('page_data_merged.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "if os.path.exists('article_data_merged.pq'):\n",
    "    article_data_merged = pd.read_parquet('article_data_merged.pq')\n",
    "elif os.path.exists('article_data'):\n",
    "    assert len(os.listdir('article_data')) >= 722, \"Delete folder 'article_data', or download missing files manually\"\n",
    "    dfs = [] \n",
    "    for file in os.listdir('article_data'):\n",
    "        dfs.append(pd.read_parquet('article_data/' + file))\n",
    "    article_data_merged = pd.concat(dfs)\n",
    "    article_data_merged.to_parquet('article_data_merged.pq')\n",
    "else: \n",
    "    dfs = get_all_articles(page_data_merged['quote_href'], header)\n",
    "    article_data_merged = pd.concat(dfs)\n",
    "    article_data_merged.to_parquet('article_data_merged.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_complete = pd.merge(page_data_merged, article_data_merged, how='left', on='quote_href')\n",
    "data_complete.to_parquet('data_complete.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline(document):\n",
    "    document = re.sub('\\n', '', document)\n",
    "    return document \n",
    "\n",
    "def description_date(document):\n",
    "    document = re.sub('stated on ', '', document)\n",
    "    document = re.findall(r'[\\w]* [\\d]+, \\d\\d\\d\\d', document)[0]\n",
    "    document = datetime.datetime.strptime(document, '%B %d, %Y')\n",
    "    return document\n",
    "\n",
    "def description_forum(document):\n",
    "    document = re.sub(r'stated on [\\w]* [\\d]+, \\d\\d\\d\\d in ', '', document)\n",
    "    document = re.sub('\\.:', '', document)\n",
    "    document = re.sub(':', '', document)\n",
    "    document = re.sub(r'^[a][n]* ', '', document)\n",
    "    return document\n",
    "\n",
    "def footer_split(document):\n",
    "    document = re.sub('By ', '', document).split('•')\n",
    "    return document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Make copy and set index\n",
    "    df = data.copy()\n",
    "    df.rename(columns={'quote_href': 'URL'}, inplace=True)\n",
    "    df.set_index('URL', inplace=True)\n",
    "\n",
    "    # Remove '\\n'\n",
    "    df['name_txt'] = df['name_txt'].apply(remove_newline)\n",
    "    df['description_txt'] = df['description_txt'].apply(remove_newline)\n",
    "    df['quote_txt'] = df['quote_txt'].apply(remove_newline)\n",
    "    df['footer'] = df['footer'].apply(remove_newline)\n",
    "    df['sub_header'] = df['sub_header'].apply(remove_newline)\n",
    "\n",
    "    # Extract description info\n",
    "    df['description_date'] = df['description_txt'].apply(description_date)\n",
    "    df['description_forum'] = df['description_txt'].apply(description_forum)\n",
    "    df.drop('description_txt', axis=1, inplace=True)\n",
    "\n",
    "    # Extract footer info\n",
    "    df['footer_split'] = df['footer'].apply(footer_split)\n",
    "    df['footer_name'] = [x[0].strip() for x in df['footer_split'] ]\n",
    "    df['footer_date'] = [datetime.datetime.strptime(x[1].strip(), '%B %d, %Y') for x in df['footer_split']]\n",
    "    df.drop(['footer_split', 'footer'], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop sources, and order and rename columns \n",
    "    df.rename(columns={'description_date'   : 'Date', \n",
    "                       'name_txt'           : 'Name',\n",
    "                       'name_href'          : 'Name URL',\n",
    "                       'description_forum'  : 'Forum',\n",
    "                       'quote_txt'          : 'Quote',\n",
    "                       'sub_header'         : 'Conclusion',\n",
    "                       'text_body'          : 'Article',\n",
    "                       'meter'              : 'Meter',\n",
    "                       'footer_date'        : 'Article date',\n",
    "                       'footer_name'        : 'Author'},\n",
    "              inplace=True)\n",
    "    df = df[['Date', 'Name', 'Name URL', 'Forum', 'Quote', 'Conclusion', 'Article', 'Meter', 'Article date', 'Author', 'tags']]\n",
    "\n",
    "    # Extract and sort tags\n",
    "    tag_values = pd.Series([x for list in df['tags'] for x in list]).value_counts()\n",
    "    few_tag_obs = tag_values[tag_values <= 50].index.values\n",
    "    many_tag_obs = tag_values[tag_values > 50].index.values\n",
    "    for tag in many_tag_obs:\n",
    "        df[f'Tag: {tag}'] = df['tags'].map(lambda x: tag in x)\n",
    "    df['Tag: Other'] = df['tags'].map(lambda x: any(tag in x for tag in few_tag_obs))\n",
    "    df.drop('tags', axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "data_clean = cleaner(data_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.to_parquet('data_clean.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.read_parquet('data_clean.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A video being shared on social media adds what may seem like a worrisome layer to the monkeypox health emergency in the United States: that the virus is being put in the water. \"Monkey pox in the water,\" someone can be heard saying while recording a news broadcast at a water reclamation facility in Fulton County, where Atlanta is the county seat. \"ATL, oh man, they put something else in the water.\" This post was flagged as part of Facebooks efforts to combat false news and misinformation on its News Feed. (Read more about our partnership with Facebook.) The July 26 news broadcast in the post reported on scientists testing wastewater for COVID-19 and the monkeypox virus to better gauge infection rates in the area. Some people have interpreted the video to mean that theres monkeypox in the Atlanta areas drinking water, and as multiple fact-checkers have noted, thats wrong. Monkeypox was detected in the wastewater there, but that doesnt mean someone put it there, or intentionally tampered with the water supply in Atlanta, as the post suggests. Rather, because the monkeypox virus can cause pus-filled blisters, running water over these sores in a shower or sink \"can catapult monkeypox DNA into wastewater,\" according to the MIT Technology Review. \"Recent data suggest that the DNA of monkeypox can also be detected in a variety of bodily fluids from those infected,\" the publication says. \"That includes respiratory and nasal secretions, spit, urine, feces and semen meaning a flushed tissue from someone with monkeypox can register the virus in wastewater.\" Even so, theres no evidence people can contract monkeypox from wastewater. Its been spreading among humans via close contact with infected persons and exposure to their rash, bodily fluids, or respiratory droplets. We rate claims that monkeypox is being put in the water False.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data_clean['Article'][1].encode('ascii', 'ignore')\n",
    "test.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(document):\n",
    "\n",
    "    # Lowercase and remove non-alphanumeric characters\n",
    "    document = document.lower()\n",
    "    document = re.sub(r'[^\\w\\s]', '', document)\n",
    "\n",
    "    # Tokenize \n",
    "    document_tokens = nltk.tokenize.word_tokenize(document)\n",
    "\n",
    "    # Delete stop-words\n",
    "    document_nostop = [i for i in document_tokens if i not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize \n",
    "    document_lemmatized = [nltk.WordNetLemmatizer().lemmatize(i) for i in document_nostop]\n",
    "    \n",
    "    # Covert from list back to string \n",
    "    document = ' '.join(document_lemmatized)\n",
    "    \n",
    "    return document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Quote clean'] = data_clean['Quote'].apply(prepare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(df, true, fake): \n",
    "    \n",
    "    part = df[df['Meter'].isin(true + fake)]\n",
    "    part['Fake'] = part['Meter'].isin(fake).astype(int)\n",
    "\n",
    "    return part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pants-fire', 'false', 'half-flip', 'half-true', 'barely-true',\n",
       "       'mostly-true', 'true', 'full-flop', 'no-flip'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['Meter'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'true' > 'mostly-true' > 'half-true' > 'barely-true' > 'false' > 'pants-fire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(['true'], ['pants-fire']),\n",
    "(['true'], ['pants-fire', 'false'])\n",
    "(['true'], ['pants-fire', 'false', 'barely-true'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/81y7szl939gbyq_6237klkch0000gn/T/ipykernel_958/4022638899.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  part['Fake'] = part['Meter'].isin(fake).astype(int)\n"
     ]
    }
   ],
   "source": [
    "test = partition(data_clean, ['true', 'mostle-true'], ['pants-fire', 'false'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = test['Quote clean'].values\n",
    "y_train = test['Fake'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_bag = tfidf.fit_transform(X_train)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=0) #Text classifier\n",
    "lr.fit(X_train_bag,y_train)\n",
    "train_preds = lr.predict(X_train_bag)\n",
    "\n",
    "features = ['_'.join(s.split()) for s in tfidf.get_feature_names_out()]\n",
    "coefficients = lr.coef_\n",
    "coefs_df = pd.DataFrame.from_records(coefficients, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>biden</th>\n",
       "      <td>3.329580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show</th>\n",
       "      <td>2.875544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccine</th>\n",
       "      <td>2.745449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid19</th>\n",
       "      <td>2.672034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photo</th>\n",
       "      <td>2.229911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joe</th>\n",
       "      <td>2.156875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>2.050232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obamacare</th>\n",
       "      <td>2.023995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coronavirus</th>\n",
       "      <td>2.015709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid</th>\n",
       "      <td>1.870258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "biden        3.329580\n",
       "show         2.875544\n",
       "vaccine      2.745449\n",
       "covid19      2.672034\n",
       "photo        2.229911\n",
       "joe          2.156875\n",
       "say          2.050232\n",
       "obamacare    2.023995\n",
       "coronavirus  2.015709\n",
       "covid        1.870258"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_df.T.nlargest(10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8277303289115028"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.mean([(train_preds==y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "kfolds = Kfold(n_split=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X_lem_values = X['ready'].values\n",
    "X_bag = TfidfVectorizer().fit_transform(X_lem_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5559e38aaa1a46293ac94fc6af3e6c13e1ce8b596ee75d4ce8712132156e5245"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
