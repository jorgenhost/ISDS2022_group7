{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** \n",
    "* Jørgen Baun Høst, pjz633@alumni.ku.dk\n",
    "* Marius Heltberg Lassen, pgb206@alumni.ku.dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the data extraction, data structuring and manipulation, calculations and figures for our project *Liar, liar, pants on fire? Fake news detection with supervised machine learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and set magics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(response: requests.Response):\n",
    "    \"\"\"\n",
    "    Creates or appends a log-file with information from a `requests.get()`-call.\n",
    "    \n",
    "    The information gathered is:\n",
    "    - - - - - - - -\n",
    "        timestamp   :   Current local time.\n",
    "        status_code :   Status code from requests call.\n",
    "        length      :   Length of the HTML-string.\n",
    "        output_path :   Current working directory.\n",
    "        url         :   The URL of the response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open or create the csv file\n",
    "    if os.path.isfile('log'):\n",
    "        log = open('log','a')\n",
    "    else: \n",
    "        log = open('log','w')\n",
    "        header = ['timestamp', 'status_code', 'length', 'output_file', 'url'] # Header names\n",
    "        log.write(';'.join(header) + \"\\n\")\n",
    "        \n",
    "    # Gather log information\n",
    "    status_code = response.status_code # Status code from the request result\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())) # Local time\n",
    "    length = len(response.text) # Length of the HTML-string\n",
    "    output_path = os.getcwd() # Output path\n",
    "    url = response.url # URL-string\n",
    "    \n",
    "    # Open the log file and append the gathered log information\n",
    "    with open('log','a') as log:\n",
    "        log.write(f'{timestamp};{status_code};{length};{output_path};{url}' + \"\\n\") \n",
    "\n",
    "\n",
    "def get_soup(url: str, header: dict) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Constructs a HTML-string from a request of the given URL. \n",
    "    Requests are logged, see `log()`. \n",
    "\n",
    "    Input:\n",
    "    - - - - - - - - \n",
    "    url (str)     :    URL of the website to receive the HTML-string from. \\n\n",
    "    header (dict) :    Dictionary to send in the query string for the request.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    soup (BeautifulSoup) :  HTML-string in the class of BeutifulSoup with 'lxml' parser.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url, headers=header) # Request\n",
    "    log(response) # Log \n",
    "    soup = BeautifulSoup(response.content, 'lxml') # Convert to response to HTML\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_articles_pf_pages(soup: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    Extracts articles from HTML-string from the PolitiFact website.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    soup (BeautifulSoup) : HTML-string from the PolitiFact website.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    list_of_articles (list) : A list of all articles in the given soup. \\n\n",
    "                              Each element is an article of data structure as BeautifulSoup.\n",
    "    \"\"\"\n",
    "    \n",
    "    articles = soup.find(class_='o-listicle__list') # Find section with articles\n",
    "    list_of_articles = articles.find_all('li') # Find all articles as a list\n",
    "\n",
    "    return list_of_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolitiFact functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_pf_pages(page: int) -> str:\n",
    "    \"\"\"\n",
    "    Creates a PolitiFact URL with the given pagenumber.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    page (int) :    Pagenumber for the PolitiFact website.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    url (str)  :    URL of the PolitiFact website for given page. \n",
    "    \"\"\"\n",
    "\n",
    "    url = f'https://www.politifact.com/factchecks/list/?page={page}' # Construct url with f-string\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def extract_info_pf_pages(article: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    Extracts all relevant information from one article on the article overview pages on the PolitiFact website.\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - - \n",
    "    article (BeautifulSoup) :  Article to extract data from, see `extract_articles()`.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - - \n",
    "    [name_txt, name_href, description_txt, quote_txt, quote_href, meter, footer] (list) \\n \n",
    "    The name and URL of the quoted person, the description of the quote, the quote itself \\n\n",
    "    and link hereof, the truthfulness index, and information on the article in string-format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Statement name \n",
    "    name = article.find(class_='m-statement__name')\n",
    "    name_txt = name.text # name \n",
    "    name_href = name['href'] # href\n",
    "\n",
    "    # Statement description\n",
    "    description_txt = article.find(class_='m-statement__desc').text\n",
    "\n",
    "    # Statement quote\n",
    "    quote = article.find(class_='m-statement__quote').a\n",
    "    quote_txt = quote.text # name \n",
    "    quote_href = quote['href'] # href\n",
    "\n",
    "    # Statement meter\n",
    "    meter = article.find(class_='m-statement__meter').div.img['alt']\n",
    "\n",
    "    # Statement footer\n",
    "    footer = article.find(class_='m-statement__footer').text\n",
    "\n",
    "    return [name_txt, name_href, description_txt, quote_txt, quote_href, meter, footer]\n",
    "\n",
    "\n",
    "def data_pf_pages(startpage: int, endpage: int, header: dict, sleep: float) -> list:\n",
    "    \"\"\"\n",
    "    Compound function that scrapes an interval of pages from PolitiFact, extracts information for analysis, \\n\n",
    "    and logs requests in `log`-file. \n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    startpage (int)  :  The first page to scrape. \\n\n",
    "    endpage   (int)  :  The last page to scrape. \\n\n",
    "    header    (dict) :  Dictionary to send in the query string for the request. \\n\n",
    "    sleep     (float):  Seconds to sleep between each request.\n",
    "\n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    list_of_output (list) : A list of lists, where each element list is the output of `extract_info()`.\n",
    "    errors         (list) : An error list containing the URLs for the pages where the error occured.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_output = [] # initialize empty list for dataframes\n",
    "    errors = [] \n",
    "\n",
    "    # Loop through pages and track progress with tqdm\n",
    "    for page in tqdm.tqdm(range(startpage, endpage+1)):\n",
    "        url = create_url_pf_pages(page) # create url\n",
    "\n",
    "        try: # circumvent problem with empty pages\n",
    "            soup = get_soup(url, header) # construct html\n",
    "            articles = extract_articles_pf_pages(soup) # extract articles \n",
    "\n",
    "            output = [] # initialize empty for articles \n",
    "\n",
    "            # Loop through articles \n",
    "            for article in articles:\n",
    "                info = extract_info_pf_pages(article) # extract relevant information\n",
    "                output.append(info) # append output\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        except: # skip page\n",
    "            print(f'Error encountered on page {page}')\n",
    "            errors.append(url)\n",
    "            time.sleep(sleep)\n",
    "            continue\n",
    "\n",
    "        list_of_output.append(output)\n",
    "\n",
    "    return list_of_output, errors\n",
    "\n",
    "\n",
    "def extract_info_pf_articles(article: BeautifulSoup) -> list:\n",
    "    \"\"\"\n",
    "    A function that scrapes each individual article for relevant data. \\n\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    article (BeautifulSoup) : BeatifulSoup element of article. \\n\n",
    "    \n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    [tags, sub_header, text_body] : A list of of all relevant data from each politifact article.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract tags\n",
    "    tag_soup = article.find(class_='m-list m-list--horizontal')\\\n",
    "        .find_all('a') #Find all tags\n",
    "    \n",
    "    list_of_tags = [] #Create empty list for tags \n",
    "    \n",
    "    for tag in tag_soup:\n",
    "        list_of_tags.append(tag['title']) #Append each tag to list of tags\n",
    "\n",
    "    # Extract sub-header\n",
    "    sub_header = article.find(class_='c-title c-title--subline').text #conclusion by journalist\n",
    "\n",
    "    # Extract entire text body\n",
    "    text_block = article.find(class_='m-textblock') #Find article's body text\n",
    "    text_body = []\n",
    "\n",
    "    for paragraph in text_block.find_all('p'): #Find all paragraphs in article\n",
    "        text_body.append(paragraph.text) #append them to list\n",
    "\n",
    "    text_body=' '.join(text_body) #Convert to a single string\n",
    "\n",
    "    return [list_of_tags, sub_header, text_body]\n",
    "\n",
    "\n",
    "def data_pf_articles(list_of_href: list, header: dict, sleep: float) -> list:\n",
    "    \"\"\"\n",
    "    A compound function that scrapes relevant data from each article on politifact.com and stores this in a DataFrame. \\n\n",
    "\n",
    "    Input:\n",
    "    - - - - - - - -\n",
    "    list_of_href (list) :  A list of PolitiFact hyperlinks for each article to scrape. \\n\n",
    "    header       (dict) :  Dictionary to send in the query string for the request. \\n\n",
    "    sleep        (float):  Seconds to sleep between each request.\n",
    "    \n",
    "    Returns:\n",
    "    - - - - - - - -\n",
    "    list_of_output (list) : A list of lists, where each element list is the output of `get_article_data()` and the article href.\n",
    "    errors         (list) : An error list containing the URLs for the pages where the error occured.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_output = [] # Initialize empty output list \n",
    "    errors = []\n",
    "\n",
    "    # Loop through list of URls\n",
    "    for article_url in tqdm.tqdm(list_of_href): \n",
    "        full_url = 'https://www.politifact.com' + article_url\n",
    "        try:   \n",
    "            article = get_soup(full_url, header=header) #Get BeautifulSoup element for each article\n",
    "            article_data = extract_info_pf_articles(article) #Extract data from article\n",
    "            article_data.append(article_url) #Quote_href\n",
    "            list_of_output.append([article_data]) #Append data to output list\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        except: \n",
    "            print(f'Error encountered at {full_url}')\n",
    "            errors.append(article_url)\n",
    "            time.sleep(sleep)\n",
    "            continue\n",
    "\n",
    "    return list_of_output, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PeaceNet functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_pd_pages(list_of_frontpage_url: str, header: dict, sleep: float) -> list:\n",
    "    set_of_article_url = set()\n",
    "    errors = set()\n",
    "    for frontpage_url in tqdm.tqdm(list_of_frontpage_url):\n",
    "        soup = get_soup(frontpage_url, header)\n",
    "        try:\n",
    "            section = soup.find(class_='jeg_main')\n",
    "            titles = section.findChildren(class_='jeg_thumb')\n",
    "            for title in titles:\n",
    "                article_url = title.a['href']\n",
    "                set_of_article_url.add(article_url) #Same approach as above\n",
    "                time.sleep(sleep)\n",
    "        except:\n",
    "            print(f'Error encountered at {frontpage_url}')\n",
    "            errors.add(frontpage_url)\n",
    "            time.sleep(sleep)\n",
    "            continue\n",
    "    return list(set_of_article_url), list(errors)\n",
    "\n",
    "\n",
    "def extract_info_pd_articles(article: BeautifulSoup) -> list:\n",
    "    \n",
    "    title=article.find(class_='jeg_post_title').text #Extract title\n",
    "    \n",
    "    paragraphs = article.find(class_='entry-content no-share')\\\n",
    "        .find_all('p') #Extract all paragraphs\n",
    "    text_body = ' '.join([par.text for par in paragraphs])\n",
    "\n",
    "    tag_soup = article.find(class_='jeg_post_tags')\\\n",
    "        .find_all('a') #Find all tags in sup\n",
    "    tags = [tag.text for tag in tag_soup]\n",
    "    \n",
    "    return [title, paragraphs, tags] #Return as lists\n",
    "\n",
    "def data_pd_fct(list_of_article_url: list, header: dict, sleep: float) -> pd.DataFrame:\n",
    "\n",
    "    list_of_output = [] # Initialize empty list for dataframes\n",
    "    errors = []\n",
    "\n",
    "    # Loop through list of URls\n",
    "    for article_url in tqdm.tqdm(list_of_article_url): \n",
    "        article_soup = get_soup(article_url, header=header).append(article_url) #Get BeautifulSoup element for each article\n",
    "\n",
    "        try:    \n",
    "            output = extract_info_pd_articles(article_soup) #Extract data from article\n",
    "            list_of_output.append(output)\n",
    "            time.sleep(sleep) #Sleep for 0.5 seconds\n",
    "        \n",
    "        except:\n",
    "            print(f'Error encountered at {article_url}')\n",
    "            errors.append(article_url)\n",
    "            time.sleep(sleep)\n",
    "            continue\n",
    "\n",
    "    return list_of_output, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make header: State names and (non-commerical/academic) intentions for data scraping\n",
    "header = {'name_1'   : 'Marius Heltberg Lassen',    'email_1' : 'pgb206@alumni.ku.dk', \n",
    "          'name_2'   : 'Jørgen Baun Høst',          'email_2' : 'pjz633@alumni.ku.dk',\n",
    "          'intention': 'Train supervised ML model for academic purposes'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape PolitiFact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file if it exists\n",
    "if os.path.exists('data/data_pf.pq'):\n",
    "    data_pf = pd.read_parquet('data/data_pf.pq')\n",
    "\n",
    "# Or download data\n",
    "else:\n",
    "    # Scrape pages\n",
    "    pages, errors_pages = data_pf_pages(1, 728, header, 0.1)\n",
    "    pages_dfs_list = [pd.DataFrame(page, columns=['name_txt', 'name_href', 'description_txt', 'quote_txt', 'quote_href', 'meter', 'footer']) for page in pages]\n",
    "    pages_df = pd.concat(pages_dfs_list)\n",
    "\n",
    "    # Scrape articles\n",
    "    articles, errors_articles = data_pf_articles(pages_df['quote_href'], header, 0.1)\n",
    "    articles_dfs_list = [pd.DataFrame(article, columns=['list_of_tags', 'sub_header', 'text_body', 'quote_href']) for article in articles]\n",
    "    articles_df = pd.concat(articles_dfs_list)\n",
    "\n",
    "    # Merge data and save\n",
    "    data_pf = pd.merge(pages_df, articles_df, how='left', on='quote_href')\n",
    "    data_pf = data_pf[~data_pf['quote_href'].isin(errors_articles)]\n",
    "    if not os.path.exists(os.getcwd() + '/data/'):\n",
    "        os.makedirs(os.getcwd() + '/data/')\n",
    "    data_pf.to_parquet('data/data_pf.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check error lists:**\n",
    "* By checking the error pages manually, we find that they are empty. Hence, we have not lost any data by skipping them.\n",
    "* By checking the error articles manually, we find that they have a different html structure. As it is only ~30 articles, we decide to drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape PeaceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file if it exists\n",
    "if os.path.exists('data/data_pd.pq'):\n",
    "    data_pd = pd.read_parquet('data/data_pd.pq')\n",
    "\n",
    "# Or download data\n",
    "else:\n",
    "    url_api = 'https://web.archive.org/cdx/search/dcx?url=peacedata.net&collapse=digest&from=20190101&to=20200930&output=json'\n",
    "    url_list_json = json.loads(requests.get(url_api).text)\n",
    "    url_list = ['https://web.archive.org/web/' + x[1] + '/' + x[2] for x in url_list_json[1:]]\n",
    "\n",
    "    pd_article_urls, page_errors = extract_urls_pd_pages(url_list, header, 0.5)\n",
    "\n",
    "    pd_articles, pd_article_errors = data_pd_fct(pd_article_urls, header, 0.5)\n",
    "    pd_articles_dfs_list = [pd.DataFrame(article, columns=['list_of_tags', 'sub_header', 'text_body', 'quote_href']) for article in pd_articles]\n",
    "    data_pd = pd.concat(pd_articles_dfs_list)\n",
    "    data_pd = data_pd['title'].drop_duplicates()\n",
    "    data_pd.to_parquet('data/data_pd.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check error lists:**\n",
    "\n",
    "We encounter quite a few errors, which is due to the inconsistency of the HTML structure of the articles. Furhtermore, the dataset contains duplicates due to Wayback Machine's way of archiving. We decide to ignore the errors and drop duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(document):\n",
    "    document = re.sub('\\n', '', document)\n",
    "    document = re.sub('\\xa0', '', document)\n",
    "    return document \n",
    "\n",
    "def description_date(document):\n",
    "    document = re.sub('stated on ', '', document)\n",
    "    document = re.findall(r'[\\w]* [\\d]+, \\d\\d\\d\\d', document)[0]\n",
    "    document = datetime.datetime.strptime(document, '%B %d, %Y')\n",
    "    return document\n",
    "\n",
    "def description_forum(document):\n",
    "    document = re.sub(r'stated on [\\w]* [\\d]+, \\d\\d\\d\\d in ', '', document)\n",
    "    document = re.sub('\\.:', '', document)\n",
    "    document = re.sub(':', '', document)\n",
    "    document = re.sub(r'^[a][n]* ', '', document)\n",
    "    return document\n",
    "\n",
    "def footer_split(document):\n",
    "    document = re.sub('By ', '', document).split('•')\n",
    "    return document "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure PolitiFact data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if list(data_pf.columns) == ['Date', 'Name', 'Name URL', 'Forum', 'Quote', 'Conclusion', 'Article', 'Meter', 'Article date', 'Author', 'Tags']:\n",
    "    pass \n",
    "\n",
    "else:\n",
    "    # Clean text\n",
    "    data_pf['name_txt'] = data_pf['name_txt'].apply(clean_text)\n",
    "    data_pf['description_txt'] = data_pf['description_txt'].apply(clean_text)\n",
    "    data_pf['quote_txt'] = data_pf['quote_txt'].apply(clean_text)\n",
    "    data_pf['footer'] = data_pf['footer'].apply(clean_text)\n",
    "    data_pf['sub_header'] = data_pf['sub_header'].apply(clean_text)\n",
    "\n",
    "    # Extract description info\n",
    "    data_pf['description_date'] = data_pf['description_txt'].apply(description_date)\n",
    "    data_pf['description_forum'] = data_pf['description_txt'].apply(description_forum)\n",
    "    data_pf.drop('description_txt', axis=1, inplace=True)\n",
    "\n",
    "    # Extract footer info\n",
    "    data_pf['footer_split'] = data_pf['footer'].apply(footer_split)\n",
    "    data_pf['footer_name'] = [x[0].strip() for x in data_pf['footer_split'] ]\n",
    "    data_pf['footer_date'] = [datetime.datetime.strptime(x[1].strip(), '%B %d, %Y') for x in data_pf['footer_split']]\n",
    "    data_pf.drop(['footer_split', 'footer'], axis=1, inplace=True)\n",
    "\n",
    "    # Drop sources, and order and rename columns \n",
    "    data_pf.rename(columns={'quote_href'         : 'URL', \n",
    "                            'description_date'   : 'Date', \n",
    "                            'name_txt'           : 'Name',\n",
    "                            'name_href'          : 'Name URL',\n",
    "                            'description_forum'  : 'Forum',\n",
    "                            'quote_txt'          : 'Quote',\n",
    "                            'sub_header'         : 'Conclusion',\n",
    "                            'text_body'          : 'Article',\n",
    "                            'meter'              : 'Meter',\n",
    "                            'footer_date'        : 'Article date',\n",
    "                            'footer_name'        : 'Author',\n",
    "                            'tags'               : 'Tags'},\n",
    "                inplace=True)\n",
    "\n",
    "    # Reorder and save \n",
    "    data_pf = data_pf[['Date', 'Name', 'Name URL', 'Forum', 'Quote', 'Conclusion', 'Article', 'Meter', 'Article date', 'Author', 'Tags']]\n",
    "    data_pf.to_parquet('data/data_pf.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 18\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 4 # set default size of plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a3d8ce103517c0eff4a4cdd3845d26e3393bafb2f4718f9c642b57a28f3097f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
